# fundamentals of gen ai
application of mathematical techniques and refined over many years of research into statistics, data science, and machine learning. 
acknowledge core concepts and principles 

## what is gen ai
**AI** imitates *human behavior* by using *machine learning* to interact with the environment and execute tasks without explicit direction on wha to output.
**Gen AI** describe a category of capabilities within AI that *create original content*

- natural language generation 
- image generation
- code generation

## what are language models
*GEN AI* apps are powered by *language models*, a specialized type of machine learning model use to NLP tasks
- determining sentiment or otherwise classifying natural language text
- summarizing text
- comparing multiple text sources for semantic similarity 
- generating new natural language

### transformer models
machine learning models for nlp have evolved over many years. today's cutting-edge language models are based on transformer architecture, which build on and extends some technique that have been proven successful in modeling vocabularies to support NLP tasks. 

transformer model are *trained with large volumes of text*, enabling them to *represent the semantic relationships* between words, and use those relationships to *determine probable sequences* of text that make sense. 

two core components: 
- an **encoder** creates semantic representations of the training vocabulary; (chatgpt: read and understand input)
- a **decoder** generates new language sequences; (chatgpt: generate an output on that understanding)

![transformer](../imgs/az_gen_ai_transformer.png)
1. the model is trained with large volume of natural language text, often source from internet or other public source of text
2. the sequences of text are broken down into *tokens (a small unit of text, for example, individual words)* and the encoder processes these tokens sequences using *attention* to determine *relationships between tokens*. 
3. the output from the encoder is a collection of *vectors (a mathematical representation of tokens)* in which each element of the vector represents a semantic attribute of the tokens. these vectors are referred to as *embeddings*
4. the decoder works on a new sequence of text tokens and uses the embeddings generated by the encoder to generate an appropriate natural language output.
5. for example, give an input sequence like 'when my dog was', the model can use the attention technique to analyze the input tokens and the semantic attributes encoded in the embeddings to predict an appropriate completion of the sentence, such as 'a puppy'  

in practice, the specific implementation of the architecture vary; bert use only encoder, GPT use only decoder. 

### tokenization
### embedding
### Attention

## using language models
## what are copilots
## ms copilot
## consideration for copilot prompts
## extending and developing copilots
## exercise - explore ms copilot
## knowledge check

# introduce to ai studio
## what is az ai studio
## how does az ai studio work
## when to use az ai studio
## exercise - explore az ai studio
## knowledge check

# responsible gen ai
## plan a responsible gen ai solution
## identify potential harms
## measure potential harms
## mitigate potential harms
## operate a responsible gen ai solution
## exercise - explore content filters in az ai studio
## knowledge check